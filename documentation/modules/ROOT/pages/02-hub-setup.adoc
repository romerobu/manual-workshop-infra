= Deploy Argo Hub Setup
include::_attributes.adoc[]

[#laboverview]
== Lab overview

In this workshop we are covering a GitOps infrastructure configuration approach. 
We will set an argo of argos and app of apps architecture to manage setup on destination clusters, and will apply day 2 operations on managed clusters.

_Argo Hub_ is a common OpenShift cluster for every user in this workshop that will be used for deploying the basic infrastructure on managed clusters using ApplicationSet. Managed clusters are a set of Single Node OpenShift, alias _SNO_, available for every user and day 2 operations will setup based on ArgoCD instance running on managed cluster.

NOTE: You will be able to connect to both _Argo Hub_ and _managed_ clusters.

Take a look at the general overview of the lab. 

.Laboratory overview
image::diagram-1.png[]

First of all, when you install GitOps operator, it will deploy a default argo instance which is intended for cluster configuration. So in this instance you will find an Application hub-setup
which is deploying and managing a second argocd instance and its configuration.

.Argo Hub instances
image::diagram-2.png[]

Then as an user lab, you will access to the second argo instance where you will have privileges enough thanks to _RBAC_ configuration to deploy an Application for applying ApplicationSet to apply configuration on your managed cluster. In this instance you can expect two ApplicationSet: _gitops-setup_ and _bootstrap-sno_.

.Argo Hub ApplicationSet 
image::diagram-3.png[]

_gitops-setup_ ApplicationSet will install the _openshift-gitops_ operator and the _argocd-infra_ instance, while the second one will generate _'N'_ Applications on managed clusters for bootrstrapping the _argocd-apps_ instance and setup its configuration.

.Argo Hub deploying on SNO
image::diagram-4.png[]

Once GitOps is deployed on your managed cluster you can start configuring day 2 operations locally in your new provisioned _argocd-infra_ instace.

.Day 2 operations and applications deploy
image::diagram-5.png[]

[#environment]
== Environment

You have access to two clusters: *_Argo Hub_* and your own *_SNO_*.

NOTE: Learn more about *https://www.redhat.com/en/blog/meet-single-node-openshift-our-smallest-openshift-footprint-edge-architectures[Sigle Node OpenShift]*, our managed cluster in this workshop. 

Let's start log in to *_Argo Hub_* cluster:

IMPORTANT: Replace *_<name>_* for your assigned cluster in this workshop. *_<pass>_* and *_<ocp-cluster-api>_* provided by the instructor at the beginning of this workshop. 

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
oc login -u user-<name> -p <pass> <ocp-cluster-api>
----  

It is possible to log in as well trough the web console, we recommend to do it because this way you will be able to jump between _Argo Hub_ and _SNO_ clusters easily.

IMPORTANT: Change _<domain>_ information, provided by the instructor at the beginning of this workshop.

- Open <hub-ocp-cluster-console> (E.g. https://console-openshift-console.apps.argo-hub.<domain>.opentlc.com/)
- Hit *_my_htpasswd_provider_*. 

.Argo Hub log in
image::hub-login-1.png[]

- Insert _user-<name>_ and _<pass>_ provided by instructor and click *_Log in_* button.

.Insert Argo Hub crentials to log in
image::hub-login-2.png[]

On top of that, once logged in to _Argo Hub_ you will only have permissions over your project and destination cluster.

The reason why _Argo Hub_ has permissions over your managed cluster is because this is configured as a destination server, given an authentication method.

In _Argo Hub_ there is a secret in _openshift-operators_ namespace for every managed cluster containing this information. There are several ways to implement this, you can generate an 
authentication token for this purpose or you can pass _kubeconfig_ file to give full control over the destination cluster.

.Managed cluster secret in Argo Hub
image::cluster-api-sno.png[]

[#hubsetup]
== Argo Hub Setup

In the _Argo Hub_ cluster you can find two argo instances, _openshift-gitops_ (default) and _argocd_. Default instance is intended to manage the hub itself, and it is in charge of
deploying the _argocd_ instance and its configuration. While _argocd_ instance is dedicated for managed clusters setup and configuration.

As _Argo Hub_ setup should be done only once before this workshop, you can log in to _openshift-gitops_ instance console.

IMPORTANT: Change _<domain>_ information, provided by the instructor at the beginning of this workshop.

- Open <openshift-gitops-instance-console> (E.g. https://openshift-gitops-server-openshift-gitops.apps.argo-hub.<domain>.opentlc.com)
- Click *_LOG IN VIA OPENSHIFT_*.

.Log in openshift-gitops console instance
image::argo-login-1.png[]

- Hit *_my_htpasswd_provider_*. 

.OpenShift credentials for openshift-gitops instance
image::hub-login-1.png[]

- Insert _user-<name>_ and _<pass>_ provided by instructor and click *_Log in_* button.

.openshift-gitops instance log in
image::hub-login-2.png[]

_openshift-gitops_ instance has view permissions, you can take a look to the objects argo is managing for deploying the second instance.

.hub-setup overview
image::hub-setup-overview.png[]

Then you can go to the code to take a look to the helm charts used.

IMPORTANT: As this configuration is already deployed you do not need to make any change, only observe the way it was done.

- Go to your _workshop-gitops-content-deploy_ repository, navigate to the _hub-setup_ Application code and see how this sets up env configuration on _in-cluster_ (https://kubernetes.default.svc):

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
cd ~/workshop-gitops-content-deploy
vi global-config/bootstrap-a/hub-setup-a.yaml
----

[.lines_7]
[.console-output]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: hub-setup
  namespace: openshift-gitops
spec:
  destination:
    namespace: openshift-gitops
    server: https://kubernetes.default.svc
  project: default
  source:
    repoURL: https://github.com/romerobu/workshop-gitops-content-deploy.git
    targetRevision: setup-sno
    path: hub-setup/charts/gitops-setup 
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
----

Check the _openshift-gitops_ console and take a look to the objects managed by argo in the _hub-setup_ Application. As you can see, using range of values while rendering charts allows to create charts for multiple destinations without much complexity.

.hub-setup Application in openshift-gitops instance console
image::hub-setup-app.png[]

For this initial setup it is only required to create _argocd_ instance, _App Projects_, _Groups_, _Role Bindings_ and _Secrets_ for cluster destinations.

Then once this second instance, _argocd_, is up and running, you can login to the argo console with your *_user-<name>_*:

IMPORTANT: Change _<domain>_ information, provided by the instructor at the beginning of this workshop.

- Open <argocd-instance-console> (E.g. https://argocd-server-openshift-operators.apps.argo-hub.<domain>.opentlc.com)
- Click *_LOG IN VIA OPENSHIFT_*.

.Log in argocd console instance
image::argo-login-1.png[]

- Hit *_my_htpasswd_provider_*. 

.OpenShift credentials for argocd instance
image::hub-login-1.png[]

- Insert user _user-<name>_ and _<pass>_ provided by instructor and click *_Log in_* button.

.argocd instance log in
image::hub-login-2.png[]

In this instance ,_argocd_, you will log in to the console as *_admin user_* of your own project while you will only have *_cluster reader_* permissions when you login to OpenShift _Argo Hub_ console.

By using _argocd_ instance you will be able to setup your own managed cluster.

Ideally you could set up every managed with a single ApplicationSet by configuring multiple cluster-definition. However because of the workshop structure you need to perform cluster scoped configurations without affecting other users, but your repository could be easily adapted to perform actions on multiple clusters.
